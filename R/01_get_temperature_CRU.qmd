---
title: "Summarising CRU-TS climatologies of GHSL urban areas"
format: html
editor_options: 
  chunk_output_type: console
---

```{r load-packages}
library(tidyverse) # CRAN v2.0.0
library(sf)        # CRAN v1.0-19
library(units)     # CRAN v0.8-5
library(stars)     # CRAN v0.6-7
library(here)      # CRAN v1.0.1
```

The `data/CRU` folder is where to put `nc` files downloaded from the CRU website (see `data/CRU/README.md` for more details). This code only runs if these files are there. If you are checking this directly from the archived repository (which does not contain a copy of the large `nc` files), you can skip this code and go directly to the script files that use its `csv` output, which is archived in this repo.

We combine the various 10-year files into one `stars` object:

```{r load-stars}
file_list <- list.files(here("data", "CRU"), full.names = TRUE, pattern = "*.nc")

cru <- read_stars(
  file_list,
  proxy = TRUE
)

st_crs(cru) <- "wgs84"
```

We load the polygons for each urban area and take their centroids:

```{r get-centroids}
coords <- read_sf(here("data", "GHSL_1000m", "GHSL_SMOD_UC_DUC_2020_polygons_V2_0_R2023A.gpkg")) |>
  st_centroid() |>
  st_transform(crs = "wgs84")
```

For each month and year, we extract the values at the urban centroid* and arrange that in a neat table:

(*taking the centroid is fine, and make calculations faster, given the coarse size of CRU-TS grid cells)

```{r cru-tbl}
cru_tbl <- st_extract(cru, at = coords) |>
  as_tibble() |>
  mutate(
    year = year(time),
    month = month(time), .before = 3
  ) |>
  left_join(coords) |>
  arrange(urban_area, year, month)
```

```{r quality-check}
ggplot(cru_tbl) +
  geom_line(aes(time, tmp)) +
  facet_wrap(~urban_area)

ggplot(cru_tbl) +
  geom_line(aes(time, stn)) + # number of stations contributing to an obs
  facet_wrap(~urban_area)
```

For St John's we have a clear drop in quality starting 2015 (fewer stations available for some areas). 
The solution is to not use the most recent values. While the decline starts in 2015, we put the cut-off at 2020 since there's still a majority of stations before.

Then, for each of the years we decide to keep, we calculate the annual values from the monthly values...

```{r make-annual-values}
clim_interval <- 30 ## typical climatology length
clim_end <- 2020

tab <- cru_tbl |>
  select(-geom) |>
  filter(year <= clim_end & year > (clim_end - clim_interval)) |>
  select(c(tmp, month, year, urban_area)) |>
  group_by(urban_area, year) |>
  nest(data = c(tmp, month)) |>
  mutate(
    annual_tmp = map(
      .x = data,
      .f = function(.x) {
        annual_tmp <- mean(.x$tmp)
      }
    )
  ) |>
  unnest(annual_tmp) |>
  select(
    urban_area, year,
    annual_tmp
  )
```

... which we then average:

```{r averages}
tab_avg <- tab |>
  group_by(urban_area) |>
  mutate(year_range_CRU = paste(min(year), max(year), sep = "-"), .after = 2) |>
  select(-year) |>
  group_by(year_range_CRU, .add = TRUE) |>
  summarise(annual_tmp = mean(annual_tmp)) |>
  ungroup()
```

We can save these values to use later:

```{r save-CRU}
write_csv(tab_avg, here("data", paste0("CRUtmp_", tab_avg$year_range_CRU[1], ".csv")))
```
